{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOFER EXPLORATION: Using this notebook to reduce the Hofer code to its barebones so we know what is necessary for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperDiagonalThresholdedLogTransform:\n",
    "    def __init__(self, nu):\n",
    "        self.b_1 = (torch.Tensor([1, 1]) / np.sqrt(2))\n",
    "        self.b_2 = (torch.Tensor([-1, 1]) / np.sqrt(2))\n",
    "        self.nu = nu\n",
    "\n",
    "    def __call__(self, dgm):\n",
    "        if len(dgm) == 0:\n",
    "            return dgm\n",
    "        \n",
    "        self.b_1 = self.b_1.to(dgm.device)\n",
    "        self.b_2 = self.b_2.to(dgm.device)\n",
    "\n",
    "        x = torch.mul(dgm, self.b_1.repeat(dgm.size(0), 1))\n",
    "        x = torch.sum(x, 1).squeeze()\n",
    "        y = torch.mul(dgm, self.b_2.repeat( dgm.size(0), 1))\n",
    "        y = torch.sum(y, 1).squeeze()\n",
    "        i = (y <= self.nu)\n",
    "        y[i] = torch.log(y[i] / self.nu)*self.nu + self.nu\n",
    "        ret = torch.stack([x, y], 1)\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = np.load('./barcodes/dim1/2000_01_SIN_data_dim1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1113, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1bb834c7f28>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer = UpperDiagonalThresholdedLogTransform(.1)\n",
    "t = transformer(torch.tensor(ex, dtype=torch.float32))\n",
    "y = t.numpy()\n",
    "plt.scatter(y[:,0], y[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(batch: [Tensor], point_dim: int=None)->tuple:\n",
    "    \"\"\"\n",
    "    This method 'vectorizes' the multiset in order to take advances of gpu processing.\n",
    "    The policy is to embed all multisets in batch to the highest dimensionality\n",
    "    occurring in batch, i.e., max(t.size()[0] for t in batch).\n",
    "    :param batch:\n",
    "    :param point_dim:\n",
    "    :return: Tensor with size batch_size x n_max_points x point_dim\n",
    "    \"\"\"\n",
    "    if point_dim is None:\n",
    "        point_dim = batch[0].size(1)\n",
    "    assert (all(x.size(1) == point_dim for x in batch if len(x) != 0))\n",
    "\n",
    "    batch_size = len(batch)\n",
    "    batch_max_points = max([t.size(0) for t in batch])\n",
    "    input_device = batch[0].device\n",
    "\n",
    "    if batch_max_points == 0:\n",
    "        # if we are here, batch consists only of empty diagrams.\n",
    "        batch_max_points = 1\n",
    "\n",
    "    # This will later be used to set the dummy points to zero in the output.\n",
    "    not_dummy_points = torch.zeros(batch_size, batch_max_points, device=input_device)\n",
    "\n",
    "    prepared_batch = []\n",
    "\n",
    "    for i, multi_set in enumerate(batch):\n",
    "        n_points = multi_set.size(0)\n",
    "\n",
    "        prepared_dgm = torch.zeros(batch_max_points, point_dim, device=input_device)\n",
    "\n",
    "        if n_points > 0:\n",
    "            index_selection = torch.tensor(range(n_points), device=input_device)\n",
    "\n",
    "            prepared_dgm.index_add_(0, index_selection, multi_set)\n",
    "\n",
    "            not_dummy_points[i, :n_points] = 1\n",
    "\n",
    "        prepared_batch.append(prepared_dgm)\n",
    "\n",
    "    prepared_batch = torch.stack(prepared_batch)\n",
    "\n",
    "    return prepared_batch, not_dummy_points, batch_max_points, batch_size\n",
    "\n",
    "\n",
    "def is_prepared_batch(input):\n",
    "    if not (isinstance(input, tuple) and len(input) == 4):\n",
    "        return False\n",
    "    else:\n",
    "        batch, not_dummy_points, max_points, batch_size = input\n",
    "        return isinstance(batch, Tensor) and isinstance(not_dummy_points, Tensor) and max_points > 0 and batch_size > 0\n",
    "\n",
    "\n",
    "def is_list_of_tensors(input):\n",
    "    try:\n",
    "        return all([isinstance(x, Tensor) for x in input])\n",
    "\n",
    "    except TypeError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def prepare_batch_if_necessary(input, point_dimension=None):\n",
    "    batch, not_dummy_points, max_points, batch_size = None, None, None, None\n",
    "\n",
    "    if is_prepared_batch(input):\n",
    "        batch, not_dummy_points, max_points, batch_size = input\n",
    "    elif is_list_of_tensors(input):\n",
    "        if point_dimension is None:\n",
    "            point_dimension = input[0].size(1)\n",
    "\n",
    "        batch, not_dummy_points, max_points, batch_size = prepare_batch(input, point_dimension)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'SLayer does not recognize input format! Expecting [Tensor] or prepared batch. Not {}'.format(input))\n",
    "\n",
    "    return batch, not_dummy_points, max_points, batch_size\n",
    "\n",
    "def parameter_init_from_arg(arg, size, default, scalar_is_valid=False):\n",
    "    if isinstance(arg, (int, float)):\n",
    "        if not scalar_is_valid:\n",
    "            raise ValueError(\"Scalar initialization values are not valid. Got {} expected Tensor of size {}.\"\n",
    "                             .format(arg, size))\n",
    "        return torch.Tensor(*size).fill_(arg)\n",
    "    elif isinstance(arg, torch.Tensor):\n",
    "        assert(arg.size() == size)\n",
    "        return arg\n",
    "    elif arg is None:\n",
    "        if default in [torch.rand, torch.randn, torch.ones, torch.ones_like]:\n",
    "            return default(*size)\n",
    "        else:\n",
    "            return default(size)\n",
    "    else:\n",
    "        raise ValueError('Cannot handle parameter initialization. Got \"{}\" '.format(arg))\n",
    "\n",
    "class SLayerExponential(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    proposed input layer for multisets [1].\n",
    "    \"\"\"\n",
    "    def __init__(self, n_elements: int,\n",
    "                 point_dimension: int=2,\n",
    "                 centers_init: Tensor=None,\n",
    "                 sharpness_init: Tensor=None):\n",
    "        \"\"\"\n",
    "        :param n_elements: number of structure elements used\n",
    "        :param point_dimension: dimensionality of the points of which the input multi set consists of\n",
    "        :param centers_init: the initialization for the centers of the structure elements\n",
    "        :param sharpness_init: initialization for the sharpness of the structure elements\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_elements = n_elements\n",
    "        self.point_dimension = point_dimension\n",
    "\n",
    "        expected_init_size = (self.n_elements, self.point_dimension)\n",
    "\n",
    "        centers_init = parameter_init_from_arg(centers_init, expected_init_size, torch.rand, scalar_is_valid=False)\n",
    "        sharpness_init = parameter_init_from_arg(sharpness_init, expected_init_size, lambda size: torch.ones(*size)*3)\n",
    "\n",
    "        self.centers = Parameter(centers_init)\n",
    "        self.sharpness = Parameter(sharpness_init)\n",
    "\n",
    "    def forward(self, input)->Tensor:\n",
    "        batch, not_dummy_points, max_points, batch_size = prepare_batch_if_necessary(input,\n",
    "                                                                                     point_dimension=self.point_dimension)\n",
    "\n",
    "\n",
    "        batch = torch.cat([batch] * self.n_elements, 1)\n",
    "\n",
    "        not_dummy_points = torch.cat([not_dummy_points] * self.n_elements, 1)\n",
    "\n",
    "        centers = torch.cat([self.centers] * max_points, 1)\n",
    "        centers = centers.view(-1, self.point_dimension)\n",
    "        centers = torch.stack([centers] * batch_size, 0)\n",
    "\n",
    "        sharpness = torch.pow(self.sharpness, 2)\n",
    "        sharpness = torch.cat([sharpness] * max_points, 1)\n",
    "        sharpness = sharpness.view(-1, self.point_dimension)\n",
    "        sharpness = torch.stack([sharpness] * batch_size, 0)\n",
    "\n",
    "        x = centers - batch\n",
    "        x = x.pow(2)\n",
    "        x = torch.mul(x, sharpness)\n",
    "        x = torch.sum(x, 2)\n",
    "        x = torch.exp(-x)\n",
    "        x = torch.mul(x, not_dummy_points)\n",
    "        x = x.view(batch_size, self.n_elements, -1)\n",
    "        x = torch.sum(x, 2)\n",
    "        x = x.squeeze()\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'SLayerExponential (... -> {} )'.format(self.n_elements)\n",
    "\n",
    "class SLayer(SLayerExponential):\n",
    "    def __init__(self, *args,  **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        warnings.warn(\"Renaming in progress. In future use SLayerExponential.\", FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\stone\\desktop\\spring2019\\computationaltopology\\project\\proj_64\\lib\\site-packages\\ipykernel_launcher.py:159: FutureWarning: Renaming in progress. In future use SLayerExponential.\n"
     ]
    }
   ],
   "source": [
    "sl = SLayer(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.5399e-04, 8.1541e-01, 3.0558e-05, 3.6511e-03, 4.8451e-07, 1.7118e+00,\n",
       "         1.6325e-09, 7.1934e-05, 1.9116e-08, 1.2151e-04, 6.3446e-01, 4.3455e-04,\n",
       "         3.3199e-09, 7.6988e-09, 3.8588e-10, 2.8627e-02, 2.6332e-04, 1.1084e-03,\n",
       "         6.5933e-02, 2.9580e-04, 7.1572e-09, 1.2694e-05, 1.1865e-02, 1.2547e-06,\n",
       "         6.0360e-04, 1.1283e-01, 7.6537e-04, 1.1485e-03, 3.8569e-06, 4.7151e-01,\n",
       "         4.8199e-07, 1.1448e-06, 6.9134e-06, 2.4575e-09, 5.4510e-03, 1.3991e-02,\n",
       "         7.7947e-03, 4.0029e-03, 1.8370e-06, 7.8974e-06, 1.6367e+00, 2.4451e-01,\n",
       "         2.2706e-08, 4.2004e-09, 2.1340e-05, 3.5129e-05, 5.4880e-02, 7.7857e-07,\n",
       "         3.6881e-05, 6.3418e-02, 2.9504e-05, 2.1616e-06, 1.3388e-07, 1.2033e-01,\n",
       "         1.1202e-03, 2.7102e-04, 2.5601e-05, 2.4994e-01, 7.6192e-03, 4.4982e-04,\n",
       "         7.3555e-02, 3.4146e-09, 2.7747e+00, 1.6962e-01, 3.8370e-04, 3.3427e-09,\n",
       "         2.5047e-05, 2.2315e-05, 3.5160e-04, 8.0645e-09, 4.6029e-03, 1.2576e+00,\n",
       "         3.2638e-01, 1.5121e-03, 1.3502e-05],\n",
       "        [7.5399e-04, 8.1541e-01, 3.0558e-05, 3.6511e-03, 4.8451e-07, 1.7118e+00,\n",
       "         1.6325e-09, 7.1934e-05, 1.9116e-08, 1.2151e-04, 6.3446e-01, 4.3455e-04,\n",
       "         3.3199e-09, 7.6988e-09, 3.8588e-10, 2.8627e-02, 2.6332e-04, 1.1084e-03,\n",
       "         6.5933e-02, 2.9580e-04, 7.1572e-09, 1.2694e-05, 1.1865e-02, 1.2547e-06,\n",
       "         6.0360e-04, 1.1283e-01, 7.6537e-04, 1.1485e-03, 3.8569e-06, 4.7151e-01,\n",
       "         4.8199e-07, 1.1448e-06, 6.9134e-06, 2.4575e-09, 5.4510e-03, 1.3991e-02,\n",
       "         7.7947e-03, 4.0029e-03, 1.8370e-06, 7.8974e-06, 1.6367e+00, 2.4451e-01,\n",
       "         2.2706e-08, 4.2004e-09, 2.1340e-05, 3.5129e-05, 5.4880e-02, 7.7857e-07,\n",
       "         3.6881e-05, 6.3418e-02, 2.9504e-05, 2.1616e-06, 1.3388e-07, 1.2033e-01,\n",
       "         1.1202e-03, 2.7102e-04, 2.5601e-05, 2.4994e-01, 7.6192e-03, 4.4982e-04,\n",
       "         7.3555e-02, 3.4146e-09, 2.7747e+00, 1.6962e-01, 3.8370e-04, 3.3427e-09,\n",
       "         2.5047e-05, 2.2315e-05, 3.5160e-04, 8.0645e-09, 4.6029e-03, 1.2576e+00,\n",
       "         3.2638e-01, 1.5121e-03, 1.3502e-05],\n",
       "        [7.5399e-04, 8.1541e-01, 3.0558e-05, 3.6511e-03, 4.8451e-07, 1.7118e+00,\n",
       "         1.6325e-09, 7.1934e-05, 1.9116e-08, 1.2151e-04, 6.3446e-01, 4.3455e-04,\n",
       "         3.3199e-09, 7.6988e-09, 3.8588e-10, 2.8627e-02, 2.6332e-04, 1.1084e-03,\n",
       "         6.5933e-02, 2.9580e-04, 7.1572e-09, 1.2694e-05, 1.1865e-02, 1.2547e-06,\n",
       "         6.0360e-04, 1.1283e-01, 7.6537e-04, 1.1485e-03, 3.8569e-06, 4.7151e-01,\n",
       "         4.8199e-07, 1.1448e-06, 6.9134e-06, 2.4575e-09, 5.4510e-03, 1.3991e-02,\n",
       "         7.7947e-03, 4.0029e-03, 1.8370e-06, 7.8974e-06, 1.6367e+00, 2.4451e-01,\n",
       "         2.2706e-08, 4.2004e-09, 2.1340e-05, 3.5129e-05, 5.4880e-02, 7.7857e-07,\n",
       "         3.6881e-05, 6.3418e-02, 2.9504e-05, 2.1616e-06, 1.3388e-07, 1.2033e-01,\n",
       "         1.1202e-03, 2.7102e-04, 2.5601e-05, 2.4994e-01, 7.6192e-03, 4.4982e-04,\n",
       "         7.3555e-02, 3.4146e-09, 2.7747e+00, 1.6962e-01, 3.8370e-04, 3.3427e-09,\n",
       "         2.5047e-05, 2.2315e-05, 3.5160e-04, 8.0645e-09, 4.6029e-03, 1.2576e+00,\n",
       "         3.2638e-01, 1.5121e-03, 1.3502e-05],\n",
       "        [7.5399e-04, 8.1541e-01, 3.0558e-05, 3.6511e-03, 4.8451e-07, 1.7118e+00,\n",
       "         1.6325e-09, 7.1934e-05, 1.9116e-08, 1.2151e-04, 6.3446e-01, 4.3455e-04,\n",
       "         3.3199e-09, 7.6988e-09, 3.8588e-10, 2.8627e-02, 2.6332e-04, 1.1084e-03,\n",
       "         6.5933e-02, 2.9580e-04, 7.1572e-09, 1.2694e-05, 1.1865e-02, 1.2547e-06,\n",
       "         6.0360e-04, 1.1283e-01, 7.6537e-04, 1.1485e-03, 3.8569e-06, 4.7151e-01,\n",
       "         4.8199e-07, 1.1448e-06, 6.9134e-06, 2.4575e-09, 5.4510e-03, 1.3991e-02,\n",
       "         7.7947e-03, 4.0029e-03, 1.8370e-06, 7.8974e-06, 1.6367e+00, 2.4451e-01,\n",
       "         2.2706e-08, 4.2004e-09, 2.1340e-05, 3.5129e-05, 5.4880e-02, 7.7857e-07,\n",
       "         3.6881e-05, 6.3418e-02, 2.9504e-05, 2.1616e-06, 1.3388e-07, 1.2033e-01,\n",
       "         1.1202e-03, 2.7102e-04, 2.5601e-05, 2.4994e-01, 7.6192e-03, 4.4982e-04,\n",
       "         7.3555e-02, 3.4146e-09, 2.7747e+00, 1.6962e-01, 3.8370e-04, 3.3427e-09,\n",
       "         2.5047e-05, 2.2315e-05, 3.5160e-04, 8.0645e-09, 4.6029e-03, 1.2576e+00,\n",
       "         3.2638e-01, 1.5121e-03, 1.3502e-05],\n",
       "        [7.5399e-04, 8.1541e-01, 3.0558e-05, 3.6511e-03, 4.8451e-07, 1.7118e+00,\n",
       "         1.6325e-09, 7.1934e-05, 1.9116e-08, 1.2151e-04, 6.3446e-01, 4.3455e-04,\n",
       "         3.3199e-09, 7.6988e-09, 3.8588e-10, 2.8627e-02, 2.6332e-04, 1.1084e-03,\n",
       "         6.5933e-02, 2.9580e-04, 7.1572e-09, 1.2694e-05, 1.1865e-02, 1.2547e-06,\n",
       "         6.0360e-04, 1.1283e-01, 7.6537e-04, 1.1485e-03, 3.8569e-06, 4.7151e-01,\n",
       "         4.8199e-07, 1.1448e-06, 6.9134e-06, 2.4575e-09, 5.4510e-03, 1.3991e-02,\n",
       "         7.7947e-03, 4.0029e-03, 1.8370e-06, 7.8974e-06, 1.6367e+00, 2.4451e-01,\n",
       "         2.2706e-08, 4.2004e-09, 2.1340e-05, 3.5129e-05, 5.4880e-02, 7.7857e-07,\n",
       "         3.6881e-05, 6.3418e-02, 2.9504e-05, 2.1616e-06, 1.3388e-07, 1.2033e-01,\n",
       "         1.1202e-03, 2.7102e-04, 2.5601e-05, 2.4994e-01, 7.6192e-03, 4.4982e-04,\n",
       "         7.3555e-02, 3.4146e-09, 2.7747e+00, 1.6962e-01, 3.8370e-04, 3.3427e-09,\n",
       "         2.5047e-05, 2.2315e-05, 3.5160e-04, 8.0645e-09, 4.6029e-03, 1.2576e+00,\n",
       "         3.2638e-01, 1.5121e-03, 1.3502e-05]], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl([t,t,t,t,t])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
